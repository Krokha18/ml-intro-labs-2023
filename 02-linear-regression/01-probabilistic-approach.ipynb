{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39cc502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a4957d",
   "metadata": {},
   "source": [
    "# Загальна постановка задач машинного навчання (навчання з учителем)\n",
    "\n",
    "Найбільш поширений клас задач у машинному навчанні - supervised learning. Зокрема, розглянемо його детальніше постановку таких задач:\n",
    "\n",
    "X - ознаки об'єктів, y - цільова змінна. В такому випадку, метою навчання є відновити відображення:\n",
    "\n",
    "$ a: x \\rightarrow y $, \n",
    "\n",
    "де $ y \\in K $ (класифікація, де K - скінченна множина класів),\n",
    "\n",
    "$ y \\in R $ (регресія, цільова змінна із множини дійсних чисел)\n",
    "\n",
    "Виникає природне питання - як оцінити розбіжність оригінального значення y та оціненого значення a(x)? Зокрема, це часто роблять функцією втрат.\n",
    "\n",
    "Нехай (x,y) - об'єкт, взятий із вибірки даних (x - числовий вектор, y - відповідь на нього). а(x) - оцінка y за даними x.\n",
    "\n",
    "$ L(a(x), y) $ - функція втрат (loss function), що оцінює розбіжність між a(x) та y. Наприклад, для задач регресії:\n",
    "\n",
    "$ L(a(x), y) = (a(x) - y)^2 $ - squared error, квадратична похибка\n",
    "\n",
    "$ L(a(x), y) = | a(x) - y | $ - absolute error, абсолютна похибка\n",
    "\n",
    "### Метод мінімізації емпіричного ризику.\n",
    "\n",
    "Зокрема, ідея полягає в тому, щоб підібрати таке відображення а, яке максимально відповідає навчальним даним (мінімізує сумарні втрати по всім об'єктам)\n",
    "\n",
    "$$ \\sum_{x_k \\in X} L(a(x_k), y_k) \\rightarrow min_{a} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e33875e",
   "metadata": {},
   "source": [
    "### Лінійна регресія\n",
    "\n",
    "В термінах, що згадані вище, можна конкретизувати задачу у випадку лінійної регресії:\n",
    "\n",
    "$$ a(x) = (\\beta, x) = \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n$$ \n",
    "\n",
    "або можна також, додати у формулу + $\\beta_0$ - вільний член (в такому випадку, вважатимемо що $x_k = (1, x_{k1}, x_{k2},..., x_{kn})$\n",
    "\n",
    "$$ a(x) = \\beta_0 1+ \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n$$ \n",
    "\n",
    "Підставимо також квадратичну функцію втрат $ L(a(x), y) = (a(x) - y)^2 $\n",
    "\n",
    "Тоді, загальна функція втрат набуває вигляд суми втрат по всім об'єктам у вибірці (отримали Метод Найменших Квадратів):\n",
    "\n",
    "$$ L(X) = \\sum_{x_k \\in X} (y_k - (\\beta, x_k))^2 \\rightarrow min_{\\beta} $$\n",
    "\n",
    "Можемо також підставити функцію втрат \"модуль\" $ L(a(x), y) = | a(x)-y | $, тоді отримаємо Метод Найменших Модулів\n",
    "\n",
    "$$ L(X) = \\sum_{x_k \\in X} |y_k - (\\beta, x_k)| \\rightarrow min_{\\beta} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435e0ed6",
   "metadata": {},
   "source": [
    "### Задача лінійної регресії має ймовірнісну природу!\n",
    "\n",
    "Припустимо, що $y_k$ можна розділити на детерміновану та стохастичну складову:\n",
    "\n",
    "$$ y_k = (\\beta, x_k) + \\epsilon_k = \\beta_1 x_{k1} + \\beta_2 x_{k2} + ... + \\beta_n x_{kn} + \\epsilon_k $$\n",
    "\n",
    "де $\\epsilon_k$ - випадкова величина, що описує відхилення від детермінованої частини.\n",
    "\n",
    "Покладемо $ (\\epsilon_1, ... \\epsilon_k) $ - незалежні між собою, однаково розподілені, мають нульове математичне сподівання.\n",
    "\n",
    "$$ \\epsilon_i = y_i - (\\beta, x_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1a8194",
   "metadata": {},
   "source": [
    "### Припустимо, що відхилення мають нормальний розподіл\n",
    "\n",
    "$$ \\epsilon_i \\sim N(0, \\sigma^2) $$\n",
    "\n",
    "Маємо також їх значення, що отримані із досліду  ( $ y_i - (\\beta, x_i) $ ) . Оцінимо правдоподібність вибірки $(\\epsilon_1, ..., \\epsilon_k ) $\n",
    "\n",
    "$$ p(\\epsilon_i, \\sigma^2) = \\cfrac{1}{\\sqrt{2\\pi\\sigma^2}} exp(-\\cfrac{(y_i - (\\beta, x_i))^2}{2\\sigma^2}) $$\n",
    "\n",
    "$$ log p (\\epsilon_i, \\sigma^2, \\beta) = - \\cfrac{log( 2\\pi)}{2} - \\cfrac{log( \\sigma^2)}{2} -\\cfrac{(y_i - (\\beta, x_i))^2}{2\\sigma^2} $$\n",
    "\n",
    "Складемо логарифм правдоподібності (це сума логарифмів щільностей)\n",
    "\n",
    "$$ logLikelihood(..., \\sigma^2, \\beta) = - k \\cfrac{log( 2\\pi)}{2} - k \\cfrac{log( \\sigma^2)}{2} - \\sum_{i=1}^k \\cfrac{(y_i - (\\beta, x_i))^2}{2\\sigma^2} \\rightarrow max_{\\sigma^2, \\beta} $$\n",
    "\n",
    "Домножимо функцію на -1 (max зміниться на min), також позбудемось від доданків $- k \\cfrac{log( 2\\pi)}{2}$ що не впливають на оптимізацію:\n",
    "\n",
    "$$  k \\cfrac{log( \\sigma^2)}{2} + \\sum_{i=1}^k \\cfrac{(y_i - (\\beta, x_i))^2}{2\\sigma^2} \\rightarrow min_{\\sigma^2, \\beta} $$\n",
    "\n",
    "Отриманий вираз дуже схожий на той, що ми ставимо у задачі найменших квадратів, тому можна сказати що задачі майже еквівалентні"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f995c9",
   "metadata": {},
   "source": [
    "### Припустимо що відхилення мають розподіл Лапласа (вивести самостійно, провести всі ті ж самі дії)\n",
    "\n",
    "$$ \\epsilon_i \\sim Laplas(0, \\sigma)  (\\sigma > 0) $$\n",
    "\n",
    "$$ p(\\epsilon_i, \\sigma, \\beta) = \\cfrac{1}{2\\sigma} exp(-\\cfrac{|y_i - (\\beta, x_i)|}{2\\sigma}) $$\n",
    "\n",
    "В цьому випадку ми маємо прийти в Метод найменших модулів"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fti_labs",
   "language": "python",
   "name": "fti_labs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
