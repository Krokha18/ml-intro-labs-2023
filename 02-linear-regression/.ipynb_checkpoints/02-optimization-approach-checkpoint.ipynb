{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10f6325",
   "metadata": {},
   "source": [
    "### Метод мінімізації емпіричного ризику.\n",
    "\n",
    "Зокрема, ідея полягає в тому, щоб підібрати таке відображення а, яке максимально відповідає навчальним даним (мінімізує сумарні втрати по всім об'єктам)\n",
    "\n",
    "$$ \\sum_{x_k \\in X} L(a(x_k), y_k) \\rightarrow min_{a} $$\n",
    "\n",
    "### Лінійна регресія\n",
    "\n",
    "В термінах, що згадані вище, можна конкретизувати задачу у випадку лінійної регресії:\n",
    "\n",
    "$$ a(x) = (\\beta, x) = \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n$$ \n",
    "\n",
    "або можна також, додати у формулу + $\\beta_0$ - вільний член (в такому випадку, вважатимемо що $x_k = (1, x_{k1}, x_{k2},..., x_{kn})$\n",
    "\n",
    "$$ a(x) = \\beta_0 1+ \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n$$ \n",
    "\n",
    "Підставимо також квадратичну функцію втрат $ L(a(x), y) = (a(x) - y)^2 $\n",
    "\n",
    "Тоді, загальна функція втрат набуває вигляд суми втрат по всім об'єктам у вибірці:\n",
    "\n",
    "$$ L(X) = \\sum_{x_k \\in X} (y_k - (\\beta, x_k))^2 \\rightarrow min_{\\beta} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfd9a55",
   "metadata": {},
   "source": [
    "# Підходи до розв'язання задачі лінійної регресії\n",
    "\n",
    "Знайдемо градієнт функції втрат, по координатам вектора параметрів ($\\beta$)\n",
    "\n",
    "$ \\frac{\\partial L}{\\partial \\beta_1} = \\sum_{x_k \\in X} 2(y_k - (\\beta, x_k))(-x_{k1})$\n",
    "\n",
    "...\n",
    "\n",
    "$  \\frac{\\partial L}{\\partial \\beta_i} = \\sum_{x_k \\in X} 2(y_k - (\\beta, x_k))(-x_{ki})$\n",
    "\n",
    "1. Градієнтний спуск\n",
    "\n",
    "- задаємо початкове наближення (наприклад, $\\beta = (0, ... 0), \\alpha = 0.1$ )\n",
    "\n",
    "- виконуємо ітерації до збіжності методу (рахуємо градієнт, оновлюємо вектор параметрів)\n",
    "\n",
    "$ \\nabla L = (\\frac{\\partial L}{\\partial \\beta_1}, .... \\frac{\\partial L}{\\partial \\beta_n}) $ - градієнт функції втрат\n",
    "\n",
    "$ \\beta = \\beta - \\alpha \\nabla L(\\beta) $ \n",
    "\n",
    "2. Нормальне рівняння\n",
    "\n",
    "$ L(X) = || X \\beta - y||^2 \\rightarrow min_{\\beta} $\n",
    "\n",
    "За правилами матричного-векторного диференціювання, візьмемо похідну по $\\beta$ і прирівняємо до нуля\n",
    "\n",
    "$ \\nabla L = \\frac{dL}{d\\beta} = 2 X^T (X \\beta - y) = 0 $\n",
    "\n",
    "$ X^T X \\beta = X^T y $\n",
    "\n",
    "$\\beta = (X^T X)^{-1} X^T y$ - розв'язок нормального рівняння"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84f62e3",
   "metadata": {},
   "source": [
    "### Порівняння підходів (град спуск vs нормальне рівняння)\n",
    "\n",
    "Давайте порівняємо два методи:\n",
    "\n",
    "-кількість ітерацій (градієнтний спуск потребує більше ітерацій)\n",
    "\n",
    "-обчислювальну складність (по перше, може не бути оберненої матриці, по друге - проблеми з високими розмірностями)\n",
    "\n",
    "$ X \\in Mat(3,2000), X^T \\in Mat(2000,3) , X^T X \\in Mat(2000,2000), (X^T X)^{-1} \\in Mat(2000,2000)$ \n",
    "\n",
    "Проблеми із оберненою матрицею виникають, коли в X є кореляції величини 1, в такому випадку $ X^T X $ - вироджена.\n",
    "\n",
    "Градієнтний спуск може викликати велику кількість ітерацій, але у сумі вони легші ніж одне обернення великої матриці в нормальному рівнянні."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fti_labs",
   "language": "python",
   "name": "fti_labs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
